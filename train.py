import yaml
import logging
import warnings
from pathlib import Path
import torch
import numpy as np
from datasets import load_dataset, Image
from sklearn.model_selection import train_test_split
from transformers import (
    AutoImageProcessor,
    TrainingArguments,
    Trainer,
    Dinov2Config,
    EvalPrediction,
    TrainerCallback
)
import evaluate
from models import Dinov2FineGrained
from dataset import CustomDataset, safe_split_dataset

# é…ç½®åŠ è½½
with open("config.yaml", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# åˆå§‹åŒ–è·¯å¾„
train_conf = config["training"]
output_dir = Path(train_conf["output_dir"])
output_dir.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(output_dir / "training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message="Was asked to gather along dimension 0")

# å®šä¹‰è¯„ä¼°æŒ‡æ ‡
metric = evaluate.load("accuracy")


class EarlyStoppingCallback(TrainerCallback):
    """è‡ªå®šä¹‰æ—©åœå›è°ƒï¼Œç›‘æ§éªŒè¯é›†æŒ‡æ ‡"""

    def __init__(self, early_stopping_patience, early_stopping_threshold, metric_name, greater_is_better):
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_threshold = early_stopping_threshold
        self.metric_name = metric_name
        self.greater_is_better = greater_is_better
        self.best_metric = None
        self.patience_counter = 0

    def on_evaluate(self, args, state, control, **kwargs):
        # ç¡®ä¿æœ‰è¯„ä¼°ç»“æœ
        if not state.log_history:
            return

        # è·å–å½“å‰æŒ‡æ ‡å€¼
        current_metrics = state.log_history[-1]
        current_metric = current_metrics.get(self.metric_name)
        if current_metric is None:
            logger.warning(f"Early stopping metric {self.metric_name} not found in evaluation results.")
            return

        # åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡
        if self.best_metric is None:
            self.best_metric = current_metric
            self.patience_counter = 0
            return

        # æ¯”è¾ƒå½“å‰æŒ‡æ ‡ä¸æœ€ä½³æŒ‡æ ‡
        if self.greater_is_better:
            improved = current_metric > (self.best_metric + self.early_stopping_threshold)
        else:
            improved = current_metric < (self.best_metric - self.early_stopping_threshold)

        if improved:
            self.best_metric = current_metric
            self.patience_counter = 0
            logger.info(f"â­ New best {self.metric_name}: {self.best_metric:.4f}")
        else:
            self.patience_counter += 1
            logger.info(
                f"ğŸš« No improvement for {self.metric_name} ({self.patience_counter}/{self.early_stopping_patience})")
            if self.patience_counter >= self.early_stopping_patience:
                logger.info("ğŸ›‘ Early stopping triggered, stopping training...")
                control.should_training_stop = True
                # åœæ­¢è¯„ä¼°å’Œä¿å­˜ä»¥ç«‹å³ç»“æŸè®­ç»ƒ
                control.should_evaluate = False
                control.should_save = False


def compute_metrics(p: EvalPrediction):
    preds = np.argmax(p.predictions, axis=1)
    return metric.compute(predictions=preds, references=p.label_ids)


def train_pipeline():
    try:
        logger.info("=== åˆå§‹åŒ–è®­ç»ƒæµç¨‹ ===")

        # åŠ è½½æ•°æ®é›†
        logger.info("åŠ è½½æ•°æ®é›†ä¸­...")
        full_dataset = load_dataset(
            "imagefolder",
            data_dir=train_conf["dataset_path"],
            split="train"
        ).cast_column("image", Image())

        # æ‹†åˆ†æ•°æ®é›†
        logger.info("æ‹†åˆ†è®­ç»ƒé›†/éªŒè¯é›†...")
        train_ds, val_ds = safe_split_dataset(
            full_dataset,
            val_ratio=train_conf["val_ratio"],
            min_samples=train_conf["min_samples"]
        )

        # åˆå§‹åŒ–å¤„ç†å™¨
        logger.info("åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨...")
        processor = AutoImageProcessor.from_pretrained(
            train_conf["model_name"],
            do_rescale=True,
            do_normalize=True
        )

        # å‡†å¤‡æ•°æ®é›†
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        train_data = CustomDataset(
            train_ds,
            processor,
            augmentation_config=train_conf["augmentation"],
            is_train=True
        )

        logger.info("å‡†å¤‡éªŒè¯æ•°æ®...")
        val_data = CustomDataset(
            val_ds,
            processor,
            is_train=False
        )

        # åˆå§‹åŒ–æ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model_config = Dinov2Config.from_pretrained(train_conf["model_name"])
        model_config.num_labels = len(train_ds.features["label"].names)
        model_config.id2label = {i: str(n) for i, n in enumerate(train_ds.features["label"].names)}
        model_config.label2id = {str(n): i for i, n in enumerate(train_ds.features["label"].names)}

        model = Dinov2FineGrained.from_pretrained(
            train_conf["model_name"],
            num_labels=model_config.num_labels,
            id2label=model_config.id2label,
            label2id=model_config.label2id
        )
        model.freeze_backbone()

        # è®­ç»ƒå‚æ•°é…ç½®ï¼ˆå®Œæ•´ç‰ˆï¼‰
        training_args = TrainingArguments(
            # === æ ¸å¿ƒè·¯å¾„ ===
            output_dir=str(output_dir),  # æ¨¡å‹å’Œæ—¥å¿—ä¿å­˜è·¯å¾„
            logging_dir=str(output_dir / "logs"),  # TensorBoardæ—¥å¿—ç›®å½•

            # === è®­ç»ƒæ§åˆ¶ ===
            num_train_epochs=train_conf["head_epochs"],  # è®­ç»ƒæ€»è½®æ•°
            per_device_train_batch_size=train_conf["batch_size"],  # å•è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡
            per_device_eval_batch_size=train_conf["batch_size"],  # å•è®¾å¤‡è¯„ä¼°æ‰¹æ¬¡

            # === ä¼˜åŒ–å™¨ä¸è°ƒåº¦ ===
            learning_rate=train_conf["optimizer"]["learning_rate"],  # åˆå§‹å­¦ä¹ ç‡
            weight_decay=train_conf["optimizer"]["weight_decay"],  # æƒé‡è¡°å‡ç³»æ•°
            warmup_ratio=train_conf["optimizer"]["warmup_ratio"],  # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
            optim=train_conf["optimizer"].get("name", "adamw_torch"),  # ä¼˜åŒ–å™¨ç±»å‹
            adam_beta1=train_conf["optimizer"].get("adam_beta1", 0.9),  # Adamå‚æ•°Î²1
            adam_beta2=train_conf["optimizer"].get("adam_beta2", 0.999),  # Adamå‚æ•°Î²2
            lr_scheduler_type=train_conf["optimizer"].get("scheduler", "linear"),  # è°ƒåº¦ç­–ç•¥

            # === è¯„ä¼°ç­–ç•¥ ===
            evaluation_strategy=train_conf.get("evaluation_strategy", "epoch"),  # epoch/steps/no
            eval_steps=train_conf.get("eval_steps", None),  # æŒ‰æ­¥è¯„ä¼°æ—¶ç”Ÿæ•ˆ
            eval_accumulation_steps=train_conf.get("eval_accumulation_steps", None),  # è¯„ä¼°æ¢¯åº¦ç´¯ç§¯

            # === ä¿å­˜ç­–ç•¥ ===
            save_strategy=train_conf.get("save_strategy", "epoch"),  # epoch/steps/no
            save_steps=train_conf.get("save_steps", None),  # æŒ‰æ­¥ä¿å­˜æ—¶ç”Ÿæ•ˆ
            save_total_limit=train_conf.get("save_total_limit", 3),  # æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°

            # === æ€§èƒ½ä¼˜åŒ– ===
            fp16=train_conf.get("fp16", torch.cuda.is_available()),  # FP16æ··åˆç²¾åº¦
            bf16=train_conf.get("bf16", False),  # BF16æ··åˆç²¾åº¦
            gradient_accumulation_steps=train_conf.get("gradient_accumulation_steps", 1),  # æ¢¯åº¦ç´¯ç§¯
            gradient_checkpointing=train_conf.get("gradient_checkpointing", False),  # æ¢¯åº¦æ£€æŸ¥ç‚¹
            torch_compile=train_conf.get("torch_compile", False),  # PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–
            dataloader_num_workers=train_conf.get("dataloader_num_workers", 8),  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
            dataloader_pin_memory=train_conf.get("dataloader_pin_memory", True),  # å†…å­˜å›ºå®š

            # === æ—¥å¿—ä¸è°ƒè¯• ===
            logging_steps=train_conf.get("logging_steps", 500),  # æ—¥å¿—è®°å½•é—´éš”
            report_to=train_conf.get("report_to", "tensorboard"),  # æ—¥å¿—åç«¯
            log_level=train_conf.get("log_level", "passive"),  # æ—¥å¿—çº§åˆ«
            disable_tqdm=train_conf.get("disable_tqdm", False),  # ç¦ç”¨è¿›åº¦æ¡

            # === æ¨¡å‹é€‰æ‹© ===
            load_best_model_at_end=train_conf.get("load_best_model_at_end", True),  # åŠ è½½æœ€ä½³æ¨¡å‹
            metric_for_best_model=train_conf.get("metric_for_best_model", "eval_accuracy"),  # æŒ‡æ ‡å
            greater_is_better=train_conf.get("greater_is_better", True),  # æŒ‡æ ‡æ–¹å‘

            # === é«˜çº§æ§åˆ¶ ===
            max_grad_norm=train_conf["optimizer"].get("max_grad_norm", 1.0),  # æ¢¯åº¦è£å‰ª
            remove_unused_columns=train_conf.get("remove_unused_columns", False),  # ä¿ç•™æœªç”¨åˆ—
            deepspeed=train_conf.get("deepspeed_config", None),  # DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„
            local_rank=train_conf.get("local_rank", -1),  # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        )

        # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        logger.info("\n=== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåˆ†ç±»å¤´ ===")
        callbacks = []
        if train_conf.get("early_stopping", {}).get("enabled", False):
            early_stopping_conf = train_conf["early_stopping"]
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=early_stopping_conf["patience"],
                    early_stopping_threshold=early_stopping_conf["threshold"],
                    metric_name=train_conf["metric_for_best_model"],
                    greater_is_better=train_conf["greater_is_better"]
                )
            )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_data,
            eval_dataset=val_data,
            compute_metrics=compute_metrics,
            callbacks=callbacks
        )
        trainer.train()

        # ç¬¬äºŒé˜¶æ®µå¾®è°ƒ
        logger.info("\n=== ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒéª¨å¹²ç½‘ç»œ ===")
        model.unfreeze_backbone()
        training_args.learning_rate = train_conf["optimizer"]["fine_tune_lr"]
        training_args.num_train_epochs = train_conf["fine_tune_epochs"]

        callbacks = []
        if train_conf.get("early_stopping", {}).get("enabled", False):
            early_stopping_conf = train_conf["early_stopping"]
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=early_stopping_conf["patience"],
                    early_stopping_threshold=early_stopping_conf["threshold"],
                    metric_name=train_conf["metric_for_best_model"],
                    greater_is_better=train_conf["greater_is_better"]
                )
            )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_data,
            eval_dataset=val_data,
            compute_metrics=compute_metrics,
            callbacks=callbacks
        )
        trainer.train()


        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        '''
        final_dir = output_dir / "final_model"
        model.save_pretrained(final_dir)
        processor.save_pretrained(final_dir)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{final_dir}")
        '''
        final_dir = output_dir / "final_model"
        # åªä¿å­˜éª¨å¹²ç½‘ç»œå’Œå¤„ç†å™¨
        model.dinov2.save_pretrained(final_dir)
        processor.save_pretrained(final_dir)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{final_dir}ï¼Œä¸åŒ…å«åˆ†ç±»å¤´")

    except Exception as e:
        logger.error(f"è®­ç»ƒæµç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        raise


if __name__ == "__main__":
    train_pipeline()
